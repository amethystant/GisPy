{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# adding classes folder to system path\n",
    "sys.path.insert(0, os.path.abspath('..') + '/gipsy')\n",
    "\n",
    "from data_reader import DataReader\n",
    "from data_reader import convert_docs\n",
    "from utils import find_mrc_word\n",
    "from gist import GIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = DataReader().load_input_files()\n",
    "df = convert_docs(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_id</th>\n",
       "      <th>p_id</th>\n",
       "      <th>sen_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Residents</td>\n",
       "      <td>resident</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>clamoring</td>\n",
       "      <td>clamor</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>see</td>\n",
       "      <td>see</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>whether</td>\n",
       "      <td>whether</td>\n",
       "      <td>SCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>virus</td>\n",
       "      <td>virus</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>has</td>\n",
       "      <td>have</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>been</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>detected</td>\n",
       "      <td>detect</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>their</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>neighborhoods</td>\n",
       "      <td>neighborhood</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>so</td>\n",
       "      <td>so</td>\n",
       "      <td>SCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>they</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>more</td>\n",
       "      <td>more</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>steps</td>\n",
       "      <td>step</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>avoid</td>\n",
       "      <td>avoid</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>any</td>\n",
       "      <td>any</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>contact</td>\n",
       "      <td>contact</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>American</td>\n",
       "      <td>american</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>researchers</td>\n",
       "      <td>researcher</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>starved</td>\n",
       "      <td>starve</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>data</td>\n",
       "      <td>datum</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>unlike</td>\n",
       "      <td>unlike</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>their</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>colleagues</td>\n",
       "      <td>colleague</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>countries</td>\n",
       "      <td>country</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>who</td>\n",
       "      <td>who</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>harnessing</td>\n",
       "      <td>harness</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>rivers</td>\n",
       "      <td>river</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>information</td>\n",
       "      <td>information</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>their</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>more</td>\n",
       "      <td>more</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>centralized</td>\n",
       "      <td>centralized</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>medical</td>\n",
       "      <td>medical</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>systems</td>\n",
       "      <td>system</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   d_id p_id sen_id token_id     token_text   token_lemma token_pos\n",
       "0     0    0      0        0      Residents      resident      NOUN\n",
       "1     0    0      0        1            are            be       AUX\n",
       "2     0    0      0        2      clamoring        clamor      VERB\n",
       "3     0    0      0        3             to            to      PART\n",
       "4     0    0      0        4            see           see      VERB\n",
       "5     0    0      0        5        whether       whether     SCONJ\n",
       "6     0    0      0        6            the           the       DET\n",
       "7     0    0      0        7          virus         virus      NOUN\n",
       "8     0    0      0        8            has          have       AUX\n",
       "9     0    0      0        9           been            be       AUX\n",
       "10    0    0      0       10       detected        detect      VERB\n",
       "11    0    0      0       11             in            in       ADP\n",
       "12    0    0      0       12          their        -PRON-       DET\n",
       "13    0    0      0       13  neighborhoods  neighborhood      NOUN\n",
       "14    0    0      0       14             so            so     SCONJ\n",
       "15    0    0      0       15           they        -PRON-      PRON\n",
       "16    0    0      0       16            can           can      VERB\n",
       "17    0    0      0       17           take          take      VERB\n",
       "18    0    0      0       18           more          more       ADJ\n",
       "19    0    0      0       19          steps          step      NOUN\n",
       "20    0    0      0       20             to            to      PART\n",
       "21    0    0      0       21          avoid         avoid      VERB\n",
       "22    0    0      0       22            any           any       DET\n",
       "23    0    0      0       23        contact       contact      NOUN\n",
       "24    0    0      0       24              .             .     PUNCT\n",
       "25    0    0      1        0       American      american       ADJ\n",
       "26    0    0      1        1    researchers    researcher      NOUN\n",
       "27    0    0      1        2            are            be       AUX\n",
       "28    0    0      1        3        starved        starve      VERB\n",
       "29    0    0      1        4            for           for       ADP\n",
       "30    0    0      1        5           data         datum      NOUN\n",
       "31    0    0      1        6              ,             ,     PUNCT\n",
       "32    0    0      1        7         unlike        unlike       ADP\n",
       "33    0    0      1        8          their        -PRON-       DET\n",
       "34    0    0      1        9     colleagues     colleague      NOUN\n",
       "35    0    0      1       10             in            in       ADP\n",
       "36    0    0      1       11          other         other       ADJ\n",
       "37    0    0      1       12      countries       country      NOUN\n",
       "38    0    0      1       13            who           who      PRON\n",
       "39    0    0      1       14            are            be       AUX\n",
       "40    0    0      1       15     harnessing       harness      VERB\n",
       "41    0    0      1       16         rivers         river      NOUN\n",
       "42    0    0      1       17             of            of       ADP\n",
       "43    0    0      1       18    information   information      NOUN\n",
       "44    0    0      1       19           from          from       ADP\n",
       "45    0    0      1       20          their        -PRON-       DET\n",
       "46    0    0      1       21           more          more       ADV\n",
       "47    0    0      1       22    centralized   centralized       ADJ\n",
       "48    0    0      1       23        medical       medical       ADJ\n",
       "49    0    0      1       24        systems        system      NOUN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['residents', 'are', 'cl', '##amo', '##ring', 'to', 'see', 'whether', 'the', 'virus', 'has', 'been', 'detected', 'in', 'their', 'neighborhoods', 'so', 'they', 'can', 'take', 'more', 'steps', 'to', 'avoid', 'any', 'contact', '.', 'american', 'researchers', 'are', 'starved', 'for', 'data', ',', 'unlike', 'their', 'colleagues', 'in', 'other', 'countries', 'who', 'are', 'harness', '##ing', 'rivers', 'of', 'information', 'from', 'their', 'more', 'centralized', 'medical', 'systems', '.', 'and', 'local', 'politicians', 'complain', 'that', 'they', 'can', 'not', 'provide', 'basic', 'information', 'on', 'the', 'spread', 'of', 'the', 'virus', 'to', 'their', 'constituents', '.', 'in', 'the', 'perennial', 'tug', '-', 'of', '-', 'war', 'between', 'privacy', 'and', 'transparency', 'in', 'the', 'united', 'states', ',', 'privacy', 'appears', 'to', 'be', 'winning', 'in', 'the', 'corona', '##virus', 'pan', '##de', '##mic', '.']\n",
      "Tokens id: [3901, 2024, 18856, 22591, 4892, 2000, 2156, 3251, 1996, 7865, 2038, 2042, 11156, 1999, 2037, 11681, 2061, 2027, 2064, 2202, 2062, 4084, 2000, 4468, 2151, 3967, 1012, 2137, 6950, 2024, 26042, 2005, 2951, 1010, 4406, 2037, 8628, 1999, 2060, 3032, 2040, 2024, 17445, 2075, 5485, 1997, 2592, 2013, 2037, 2062, 22493, 2966, 3001, 1012, 1998, 2334, 8801, 17612, 2008, 2027, 2064, 2025, 3073, 3937, 2592, 2006, 1996, 3659, 1997, 1996, 7865, 2000, 2037, 24355, 1012, 1999, 1996, 14638, 12888, 1011, 1997, 1011, 2162, 2090, 9394, 1998, 16987, 1999, 1996, 2142, 2163, 1010, 9394, 3544, 2000, 2022, 3045, 1999, 1996, 21887, 23350, 6090, 3207, 7712, 1012]\n",
      "Tokens PyTorch: tensor([[  101,  3901,  2024, 18856, 22591,  4892,  2000,  2156,  3251,  1996,\n",
      "          7865,  2038,  2042, 11156,  1999,  2037, 11681,  2061,  2027,  2064,\n",
      "          2202,  2062,  4084,  2000,  4468,  2151,  3967,  1012,  2137,  6950,\n",
      "          2024, 26042,  2005,  2951,  1010,  4406,  2037,  8628,  1999,  2060,\n",
      "          3032,  2040,  2024, 17445,  2075,  5485,  1997,  2592,  2013,  2037,\n",
      "          2062, 22493,  2966,  3001,  1012,  1998,  2334,  8801, 17612,  2008,\n",
      "          2027,  2064,  2025,  3073,  3937,  2592,  2006,  1996,  3659,  1997,\n",
      "          1996,  7865,  2000,  2037, 24355,  1012,  1999,  1996, 14638, 12888,\n",
      "          1011,  1997,  1011,  2162,  2090,  9394,  1998, 16987,  1999,  1996,\n",
      "          2142,  2163,  1010,  9394,  3544,  2000,  2022,  3045,  1999,  1996,\n",
      "         21887, 23350,  6090,  3207,  7712,  1012,   102]])\n",
      "Token wise output: torch.Size([1, 107, 1024]), Pooled output: torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "# creating list of tokens\n",
    "doc_context = []\n",
    "verbs_indices = []\n",
    "for index, row in df.iterrows():\n",
    "    doc_context.append(row['token_text'])\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = AutoModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "doc_string = ' '.join(doc_context)\n",
    "tokens = tokenizer.tokenize(doc_string)\n",
    "print(\"Tokens: {}\".format(tokens))\n",
    "\n",
    "# This is not sufficient for the model, as it requires integers as input, \n",
    "# not a problem, let's convert tokens to ids.\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens id: {}\".format(tokens_ids))\n",
    "\n",
    "# Add the required special tokens\n",
    "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "\n",
    "# We need to convert to a Deep Learning framework specific format, let's use PyTorch for now.\n",
    "tokens_pt = torch.tensor([tokens_ids])\n",
    "print(\"Tokens PyTorch: {}\".format(tokens_pt))\n",
    "\n",
    "# Now we're ready to go through BERT with out input\n",
    "outputs, pooled = model(tokens_pt)\n",
    "print(\"Token wise output: {}, Pooled output: {}\".format(outputs.shape, pooled.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = outputs[0]\n",
    "token_embeddings = []\n",
    "i = 0\n",
    "# creating a list of tokens and their embeddings\n",
    "while i < len(tokens):\n",
    "    token_embeddings.append([tokens[i], last_hidden_states[i+1]])\n",
    "    i += 1\n",
    "assert len(tokens) == len(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4427109473408797\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import statistics\n",
    "import torch.nn as nn\n",
    "\n",
    "cosine = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "i = 0\n",
    "i_token = 0\n",
    "verb_embeddings = []\n",
    "pos_tags = ['VERB']\n",
    "while i < len(df): \n",
    "    if df.iloc[i]['token_pos'] in pos_tags:\n",
    "        # true, if there's no sub-token\n",
    "        if df.iloc[i]['token_text'].lower() == token_embeddings[i_token][0].lower():\n",
    "            verb_embeddings.append(token_embeddings[i_token][1])\n",
    "            i += 1\n",
    "            i_token += 1\n",
    "        # it means that there are sub-tokens\n",
    "        else:\n",
    "            # if you want to check the tokens\n",
    "            # print(df.iloc[i]['token_text'], tokens[i_token])\n",
    "            tensors = [token_embeddings[i_token][1]]\n",
    "            j = copy.deepcopy(i_token) + 1\n",
    "            \n",
    "            # getting embeddings of all sub-tokens of current token and then computing their mean\n",
    "            while j < len(tokens) and '#' in tokens[j]:\n",
    "                tensors.append(token_embeddings[j][1])\n",
    "                j += 1\n",
    "            verb_embeddings.append(torch.mean(torch.stack(tensors), dim=0))\n",
    "            i += 1\n",
    "            i_token = copy.deepcopy(j)\n",
    "    else:\n",
    "        i += 1\n",
    "        i_token += 1\n",
    "\n",
    "# checking if we have the embeddings of all VERBs\n",
    "assert len(df.loc[df['token_pos'].isin(pos_tags)]), len(verb_embeddings)\n",
    "\n",
    "# computing the cosine similarity among all VERBs\n",
    "scores = []\n",
    "for pair in itertools.combinations(verb_embeddings, r=2):\n",
    "    scores.append(cosine(pair[0], pair[1]).item())\n",
    "print(statistics.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_embeddings[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[0][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(last_hidden_states[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(doc_context, return_tensors=\"pt\", is_pretokenized=True, add_special_tokens=False)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gipsy",
   "language": "python",
   "name": "gipsy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

We conducted a cross-sectional, web-based survey of 1095 adults in the United States in August 2014. Research Now12 provided the sample from a combination of online research panel members (n = 805) and a convenience river sample of Internet users (individuals who were invited to participate when they visited general, social media, and loyalty websites) (n = 290). The survey included brief embedded animated videos, developed by the study team along with Booster Shot Media,13 to explain basic concepts about ROMP. The animated videos posed an example of doctors wanting to compare three effective and commonly prescribed antihypertensive medications to discover which is best. The first video conveyed that different doctors might treat the same patient with different medications. The second video described two research methods—randomization and medical record review—that could be used to compare the medications. After viewing the videos, respondents answered survey questions about their attitudes about ROMP. The videos were iteratively refined by the study team, with input from patient focus groups. We designed the survey questions based on a series of focus groups in which we qualitatively assessed patients’ views about ROMP,14 and we refined the survey using a series of cognitive interviews with members of the public to ensure that the questions were clearly written. The videos and survey instrument are available on the ROMP Ethics Study website.15 Further details about survey development and administration are described elsewhere.16 The University of Washington and Stanford University institutional review boards approved this study.
After respondents had viewed the videos, the survey presented three hypothetical examples of ROMP: a medical record review study of antihypertensive medications, a randomized study of antihypertensive medications, and a randomized study of medications for a “more serious condition” described as causing an increased risk of stroke. Each scenario was followed by a question asking respondents whether they would be willing to consider participating in the study described. For the first and second scenarios, we also asked respondents to imagine that they were the medical decision-maker for a family member and to state whether they would be willing to consider giving permission for their family member to participate in the study. For the questions about randomized studies of hypertension when the patient is oneself, hypertension when the patient is a family member, and a more serious condition when the patient is oneself, we offered respondents the opportunity to write an open-ended response about why they would or would not consider participating. Table 1 shows the wording of each scenario and the subsequent questions (all tables for this article are available through the IRB: Ethics & Human Research web page). We limited open-ended responses to these three scenarios to minimize the burden on survey respondents. We also asked a series of standard demographic questions at the end of the survey
One author (DMK) preliminarily reviewed and cleaned all the open-ended responses, removing nonresponsive or nonsensical responses. A total of 113 such responses were deleted from the hypertension-self scenario, 135 from the hypertension– family member scenario, and 154 from the more-serious-condition–self scenario. A subgroup of the authors (KMP, DMK, MC, and CJ) used a conventional content analysis approach17 to inductively develop a codebook based on initial review of respondents’ open-ended answers, which the entire study team iteratively reviewed and revised. Two coders (SAK and KMP) subsequently underwent a training process wherein they applied the codebook to a subset of respondent answers, stratified by question and by response of willing or unwilling, and made additional revisions to finalize the codebook (see the Appendix, available through the IRB: Ethics & Human Research web page). The two coders then each independently coded half of the respondent answers, stratified proportionally based on question and response. Inter-rater reliability was calculated on 20% of all respondent answers, resulting in a Cohen’s kappa of 0.84. Coding, inter-rater reliability testing, and calculation of kappa were done using Dedoose qualitative software.18 Basic descriptive statistics were calculated using Microsoft Excel.